import '../models/vocabulary_word.dart';

// 人工知能ジャンルの単語データ
List<VocabularyWord> getAIWords() {
  return [
    VocabularyWord(
      word: 'algorithm',
      pronunciation: 'ǽlgərìðm',
      meaning: 'アルゴリズム、演算手順',
      etymology: 'アラビアの数学者 al-Khwarizmi の名前に由来。9世紀のペルシャの学者が代数学の基礎を築いた。',
      mnemonic: '「アル（al）ゴリ（gori）ズム（thm）」→ アルゴリズムはそのまま日本語でも使用される',
      examples: [
        'Machine learning algorithms can recognize patterns in data.',
        '機械学習アルゴリズムはデータのパターンを認識できる。',
        'This sorting algorithm is very efficient.',
        'このソートアルゴリズムは非常に効率的だ。',
      ],
      synonyms: ['procedure', 'method', '手順', 'アルゴリズム'],
      category: '人工知能',
    ),
    VocabularyWord(
      word: 'neural network',
      pronunciation: 'n(j)úːrəl nétwə̀ːrk',
      meaning: 'ニューラルネットワーク、神経回路網',
      etymology: 'neural（神経の）+ network（ネットワーク）。生物の神経系を模倣した計算モデル。',
      mnemonic: '「ニューラル（neural = 神経）ネットワーク」→ 脳の神経細胞のネットワークを模倣',
      examples: [
        'Deep neural networks have revolutionized AI.',
        '深層ニューラルネットワークはAIに革命をもたらした。',
        'The neural network learned to classify images.',
        'ニューラルネットワークは画像分類を学習した。',
      ],
      synonyms: ['artificial neural network', 'ANN', 'ニューラルネット'],
      category: '人工知能',
    ),
    VocabularyWord(
      word: 'transformer',
      pronunciation: 'trænsfɔ́ːrmər',
      meaning: 'トランスフォーマー（注意機構ベースのAIモデル）',
      etymology: 'transform（変換する）+ -er（するもの）。データを変換・処理するアーキテクチャ。',
      mnemonic: '「トランス（trans = 超えて）フォーム（form = 形）」→ データの形を変換する',
      examples: [
        'Transformers are the foundation of modern language models.',
        'トランスフォーマーは現代の言語モデルの基礎である。',
        'GPT uses a transformer architecture.',
        'GPTはトランスフォーマーアーキテクチャを使用している。',
      ],
      synonyms: ['attention mechanism', 'self-attention model', '変換器'],
      category: '人工知能',
    ),
    VocabularyWord(
      word: 'training',
      pronunciation: 'tréiniŋ',
      meaning: 'トレーニング、学習（機械学習における）',
      etymology: 'train（訓練する）+ -ing。AIモデルにデータを学習させるプロセス。',
      mnemonic: '「トレイン（train = 列車/訓練）イング」→ AIを訓練する過程',
      examples: [
        'The model requires extensive training on large datasets.',
        'このモデルは大規模データセットでの広範なトレーニングが必要だ。',
        'Training time can be reduced with better hardware.',
        'より良いハードウェアでトレーニング時間を短縮できる。',
      ],
      synonyms: ['learning', 'model training', '学習', '訓練'],
      category: '人工知能',
    ),
    VocabularyWord(
      word: 'inference',
      pronunciation: 'ínfərəns',
      meaning: '推論、推定（AIモデルによる予測）',
      etymology: 'infer（推論する）+ -ence（名詞語尾）。学習済みモデルが新しいデータに対して予測を行うこと。',
      mnemonic: '「イン（in = 中に）ファー（fer = 運ぶ）エンス」→ データから結論を導き出す',
      examples: [
        'Inference is much faster than training.',
        '推論はトレーニングよりもはるかに高速だ。',
        'The model performs inference in real-time.',
        'このモデルはリアルタイムで推論を実行する。',
      ],
      synonyms: ['prediction', 'reasoning', '推論', '予測'],
      category: '人工知能',
    ),
    VocabularyWord(
      word: 'backpropagation',
      pronunciation: 'bǽkprɑ̀pəgéiʃən',
      meaning: '誤差逆伝播法',
      etymology: 'back（後ろへ）+ propagation（伝播）。ニューラルネットワークの学習アルゴリズム。',
      mnemonic: '「バック（back = 戻る）プロパゲーション（propagation = 伝播）」→ 誤差を逆方向に伝える',
      examples: [
        'Backpropagation is essential for training neural networks.',
        '誤差逆伝播法はニューラルネットワークの訓練に不可欠だ。',
        'The algorithm uses backpropagation to update weights.',
        'このアルゴリズムは誤差逆伝播法を使用して重みを更新する。',
      ],
      synonyms: ['backprop', 'error backpropagation', '逆伝播'],
      category: '人工知能',
    ),
    VocabularyWord(
      word: 'embedding',
      pronunciation: 'imbédiŋ',
      meaning: '埋め込み、エンベディング（データの低次元表現）',
      etymology: 'embed（埋め込む）+ -ing。高次元データを低次元空間に写像する技術。',
      mnemonic: '「エンベッド（embed = 埋め込む）イング」→ データを空間に埋め込む',
      examples: [
        'Word embeddings capture semantic relationships.',
        '単語埋め込みは意味的関係を捉える。',
        'The model uses learned embeddings for input representation.',
        'このモデルは入力表現に学習済み埋め込みを使用する。',
      ],
      synonyms: ['vector representation', 'encoding', 'ベクトル表現'],
      category: '人工知能',
    ),
    VocabularyWord(
      word: 'overfitting',
      pronunciation: 'òuvərfítiŋ',
      meaning: '過学習、過適合',
      etymology: 'over-（過度に）+ fit（適合する）+ -ing。訓練データに過度に適合してしまう現象。',
      mnemonic: '「オーバー（over = 過度）フィッティング（fitting = 適合）」→ 学習しすぎて汎化できない',
      examples: [
        'Overfitting occurs when the model memorizes training data.',
        '過学習はモデルが訓練データを暗記した時に起こる。',
        'Regularization techniques can prevent overfitting.',
        '正則化技術は過学習を防ぐことができる。',
      ],
      synonyms: ['overtraining', '過適合', '過剰適合'],
      category: '人工知能',
    ),
    VocabularyWord(
      word: 'optimization',
      pronunciation: 'ɑ̀ptəməzéiʃən',
      meaning: '最適化',
      etymology: 'optimal（最適な）+ -ization（化）。モデルのパラメータを最良の状態に調整すること。',
      mnemonic: '「オプティマル（optimal = 最適）ゼーション（-ization = 化）」→ 最適な状態にする',
      examples: [
        'Optimization algorithms minimize the loss function.',
        '最適化アルゴリズムは損失関数を最小化する。',
        'Adam is a popular optimization method.',
        'Adamは人気のある最適化手法だ。',
      ],
      synonyms: ['tuning', 'parameter optimization', '最適化', 'パラメータ調整'],
      category: '人工知能',
    ),
    VocabularyWord(
      word: 'generative',
      pronunciation: 'dʒénəreitiv',
      meaning: '生成的な、ジェネレーティブ',
      etymology: 'generate（生成する）+ -ive（形容詞語尾）。新しいデータを生成する能力を持つ。',
      mnemonic: '「ジェネレート（generate = 生成）イブ（-ive = ～的）」→ 生成する能力がある',
      examples: [
        'Generative AI can create new images from text.',
        '生成AIはテキストから新しい画像を作成できる。',
        'ChatGPT is a generative language model.',
        'ChatGPTは生成的言語モデルだ。',
      ],
      synonyms: ['creative', 'productive', '生成的', '創造的'],
      category: '人工知能',
    ),
  ];
}
